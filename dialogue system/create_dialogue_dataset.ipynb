{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "st = StanfordNERTagger('stanford-ner-4.0.0/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                       'stanford-ner-4.0.0/stanford-ner.jar',\n",
    "                       encoding='utf-8')\n",
    "\n",
    "\n",
    "def replace_tags(text):\n",
    "    \n",
    "    tokenized_text = word_tokenize(text)\n",
    "    classified_text = st.tag(tokenized_text)\n",
    "    \n",
    "    person = [pair[0] for pair in classified_text if pair[1] == 'PERSON']\n",
    "    location = [pair[0] for pair in classified_text if pair[1] == 'LOCATION']\n",
    "    organization = [pair[0] for pair in classified_text if pair[1] == 'ORGANIZATION']\n",
    "        \n",
    "        \n",
    "    text = text.replace('<', '').replace('>', '')\n",
    "    for p in person:\n",
    "        text = text.replace(p, '<person>')\n",
    "    for l in location:\n",
    "        text = text.replace(l, '<location>')\n",
    "    for o in organization:\n",
    "        text = text.replace(o, '<organization>')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text) # exclude content between () and []\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\" re \", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'em\", \" them\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"'cuz\", \"because\", text)\n",
    "    text = re.sub(r\"'cos\", \"because\", text)\n",
    "    text = re.sub(r\"wanna\", \"want\", text)\n",
    "    text = re.sub(r\"d'you\", \"do you\", text)\n",
    "    text = re.sub(r\"d'ya\", \"do you\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"gimme\", \"give me\", text)\n",
    "    text = re.sub(r\"lemme\", \"let me\", text)\n",
    "    text = re.sub(r\"gonna\", \"going to\", text)    \n",
    "    text = re.sub(r\"ya\", \"you\", text)\n",
    "    text = re.sub(r\"yknow\", \"you know\", text)\n",
    "    text = re.sub(r\"\\x96\", \"\", text)\n",
    "    text = re.sub(r\"\\x91\", \"\", text)\n",
    "    text = re.sub(r\"you�re\", \"you are\", text)\n",
    "    text = re.sub(r\"don�t\", \"do not\", text)\n",
    "    text = re.sub(r\"it�s\", \"it is\", text)\n",
    "    text = re.sub(r\"i�m\", \"i am\", text)\n",
    "    text = re.sub(r\"that�s\", \"that is\", text)\n",
    "    text = re.sub(r\"what�s\", \"what is\", text)\n",
    "    text = re.sub(r\"didn�t\", \"did not\", text)\n",
    "    text = re.sub(r\"he�s\", \"he is\", text)\n",
    "    text = re.sub(r\"�\", \"\", text)\n",
    "    text = re.sub(r\"maam\", \"madam\", text)\n",
    "    text = re.sub(r\"kinda\", \"kind of\", text)\n",
    "    text = re.sub(r\"sorta\", \"sort of\", text)\n",
    "    text = re.sub(r\"outta\", \"out of\", text)\n",
    "    text = re.sub(r\"dunno\", \"do not know\", text)\n",
    "    text = re.sub(r\"betcha\", \"bet you\", text)\n",
    "    text = re.sub(r\"gotcha\", \"got you\", text)\n",
    "    text = re.sub(r\"coulda\", \"could have\", text)\n",
    "    text = re.sub(r\"woulda\", \"would have\", text)\n",
    "    text = re.sub(r\"cuppa\", \"cup of\", text)\n",
    "    text = re.sub(r\"whassup\", \"what is up\", text)\n",
    "    text = re.sub(r\"-\", \" \", text) \n",
    "    text = re.sub(r\"[-¯()\\\"#/@;:<>{}`+=~|,']\", \"\", text) # remove special characters. keep ponctuation (.!?)\n",
    "    text = re.sub(r\"cmon\", \"come on\", text)\n",
    "    text = re.sub(r\"  \", \" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seinfeld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://data.world/juanjosecas/seinfeld-scripts\n",
    "seinfeld = pd.read_csv('comedy_data/seinfeld_scripts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/catarina/.convokit/downloads/friends-corpus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(67373, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from convokit import Corpus, download\n",
    "corpus = Corpus(filename=download(\"friends-corpus\"))\n",
    "\n",
    "corpus_df = corpus.get_utterances_dataframe()\n",
    "corpus_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(series):\n",
    "    return series.str.split(' ').apply(len)\n",
    "\n",
    "def build_reply(df, scene_key, line_col):\n",
    "    # a dialogue reply is the next dialogue in the same scene\n",
    "    df_ = df.copy()\n",
    "    df_['text_reply'] = df_.groupby(scene_key)[line_col].shift(-1)\n",
    "    df_ = df_.dropna(subset = ['text_reply'])\n",
    "    return df_\n",
    "    \n",
    "def filter_long_dialogues(df, line_col_len, reply_col_len, thr):\n",
    "    df_ = df.copy()\n",
    "    df_['length_is_ok'] = ((df_[line_col_len] <= thr) & \n",
    "                           (df_[line_col_len] > 1) &\n",
    "                           (df_[reply_col_len] <= thr) &\n",
    "                           (df_[reply_col_len] > 1))\n",
    "    df_ = df_[df_.length_is_ok]\n",
    "    return df_.drop(['length_is_ok'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset_friends(df, thr):\n",
    "    \n",
    "    df_ = df.copy()\n",
    "    \n",
    "    # clean and uniformize text\n",
    "    df_.text = df_.text.apply(replace_tags)\n",
    "    df_.text = df_.text.apply(clean_text)\n",
    "    \n",
    "    # build reply\n",
    "    replies = df_[['reply_to','text']].rename(columns = {'text': 'text_reply', 'reply_to': 'id'})\n",
    "    replies = replies.reset_index(drop = True).dropna()\n",
    "\n",
    "    df_ = df_.reset_index().dropna(subset = ['text'])\n",
    "    df_ = df_.merge(replies, on = 'id', how = 'inner')\n",
    "    \n",
    "    \n",
    "    # calculate number of words in each line and response \n",
    "    df_['line_text_len'] = count_words(df_.text)\n",
    "    df_['line_reply_len'] = count_words(df_.text_reply)\n",
    "    \n",
    "    # filter out the dialogues with lengths that exceed the threshold\n",
    "    df_ = filter_long_dialogues(df_, 'line_text_len', 'line_reply_len', thr)\n",
    "    \n",
    "    df_['show'] = 'friends'\n",
    "    \n",
    "    df_ = df_.rename(columns = {'id':'line_id'})\n",
    "    \n",
    "    # select relevant columns\n",
    "    columns = [\n",
    "        'show',\n",
    "        'line_id',\n",
    "        'text',\n",
    "        'text_reply',\n",
    "        'line_text_len',\n",
    "        'line_reply_len'\n",
    "    ]\n",
    "\n",
    "    return df_[columns]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-58e695e68ebb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfriends_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_dataset_friends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfriends_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9745a8437e74>\u001b[0m in \u001b[0;36mclean_dataset_friends\u001b[0;34m(df, thr)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# clean and uniformize text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/virtual-environments/venv1/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9d93c5628653>\u001b[0m in \u001b[0;36mreplace_tags\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mclassified_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mperson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassified_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'PERSON'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/virtual-environments/venv1/lib/python3.6/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# This function should return list of tuple rather than list of list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/virtual-environments/venv1/lib/python3.6/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# Run the tagger and get the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         stanpos_output, _stderr = java(\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasspath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stanford_jar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         )\n\u001b[1;32m    116\u001b[0m         \u001b[0mstanpos_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanpos_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/virtual-environments/venv1/lib/python3.6/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;31m# Check the return code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communication_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1512\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "friends_clean = clean_dataset_friends(corpus_df, thr = 20)\n",
    "friends_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://data.world/abhinavr8/the-office-scripts-dataset\n",
    "the_office = pd.read_csv('comedy_data/the_office_scripts.csv')\n",
    "    \n",
    "\n",
    "def clean_dataset_office(df, thr):\n",
    "    \n",
    "    df_ = df.copy()\n",
    "    \n",
    "    # clean and uniformize text\n",
    "    df_.line_text = df_.line_text.apply(clean_text)\n",
    "\n",
    "    # create key for scenes\n",
    "    df_['scene_key'] = ('s' + df_.season.astype(str).str.zfill(2) + '_' +\n",
    "                        'e' + df_.episode.astype(str).str.zfill(2) + '_' +\n",
    "                        'c' + df_.scene.astype(str).str.zfill(2))\n",
    "    \n",
    "    \n",
    "    # create index for dialogue order\n",
    "    df_['dialogue_order'] = df_.groupby('scene_key')['line_text'].cumcount()\n",
    "    \n",
    "    # create key for dialogue utterance\n",
    "    df_['line_id'] = df_['scene_key'] + '_u' + df_.dialogue_order.astype(str).str.zfill(2)\n",
    "\n",
    "\n",
    "    # build responses for each dialogue line (response is next dialogue)\n",
    "    df_ = build_reply(df_, 'scene_key', 'line_text')\n",
    "    \n",
    "    \n",
    "    # remove dialogues with typos\n",
    "    df_['has_typo'] = (df_.line_text.str.contains('���')) | (df_.text_reply.str.contains('���'))\n",
    "    df_ = df_[~df_.has_typo].reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "    # calculate number of words in each line and response \n",
    "    df_['line_text_len'] = count_words(df_.line_text)\n",
    "    df_['line_reply_len'] = count_words(df_.text_reply)\n",
    "    \n",
    "    # filter out the dialogues with lengths that exceed the threshold\n",
    "    df_ = filter_long_dialogues(df_, 'line_text_len', 'line_reply_len', thr)\n",
    "    \n",
    "    df_['show'] = 'the_office'\n",
    "    \n",
    "    # select relevant columns\n",
    "    columns = [\n",
    "        'show',\n",
    "        'season',\n",
    "        'episode',\n",
    "        'scene',\n",
    "        'scene_key',\n",
    "        'dialogue_order',\n",
    "        'line_id',\n",
    "        'line_text',\n",
    "        'text_reply',\n",
    "        'line_text_len',\n",
    "        'line_reply_len'\n",
    "    ]\n",
    "\n",
    "    return df_[columns].rename(columns = {'line_text':'text'})\n",
    "    \n",
    "    \n",
    "the_office_clean = clean_dataset_office(the_office, thr = 20)\n",
    "the_office_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_office_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### himym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "himym = pd.read_csv('comedy_data/himym.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "himym.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gilmore Girls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = pd.read_csv('comedy_data/gilmore_girls.csv')\n",
    "\n",
    "\n",
    "def clean_dataset_gg(df, thr):\n",
    "    \n",
    "    df_ = df.copy()\n",
    "    \n",
    "    df_ = df_.drop('scene', axis = 1).rename(columns = {'dialogues':'scene', 'sequence':'dialogue_order'})\n",
    "    df_['season'] = df_.episode.apply(lambda text: int(text.split('x')[0]))\n",
    "    df_['episode'] = df_.episode.apply(lambda text: int(text.split('x')[1].split('-')[0]))\n",
    "    \n",
    "\n",
    "    \n",
    "    # clean and uniformize text\n",
    "    df_['text'] = df_['line'].fillna('')\n",
    "    df_['text'] = df_.text.apply(clean_text)\n",
    "\n",
    "    # create key for scenes\n",
    "    df_['scene_key'] = ('s' + df_.season.astype(str).str.zfill(2) + '_' +\n",
    "                        'e' + df_.episode.astype(str).str.zfill(2) + '_' +\n",
    "                        'c' + df_.scene.astype(str).str.zfill(2))\n",
    "    \n",
    "    # create key for dialogue utterance\n",
    "    df_['line_id'] = df_['scene_key'] + '_u' + df_.dialogue_order.astype(str).str.zfill(2)\n",
    "    \n",
    "    df_ = df_.sort_values(by = ['season', 'episode', 'scene', 'dialogue_order'])\n",
    "    \n",
    "    # build responses for each dialogue line (response is next dialogue)\n",
    "    df_ = build_reply(df_, 'scene_key', 'text')\n",
    "    \n",
    "    \n",
    "    # calculate number of words in each line and response \n",
    "    df_['line_text_len'] = count_words(df_.text)\n",
    "    df_['line_reply_len'] = count_words(df_.text_reply)\n",
    "    \n",
    "    # filter out the dialogues with lengths that exceed the threshold\n",
    "    df_ = filter_long_dialogues(df_, 'line_text_len', 'line_reply_len', thr)\n",
    "    \n",
    "    df_['show'] = 'gilmore_girls'\n",
    "    \n",
    "    # select relevant columns\n",
    "    columns = [\n",
    "        'show',\n",
    "        'season',\n",
    "        'episode',\n",
    "        'scene',\n",
    "        'scene_key',\n",
    "        'dialogue_order',\n",
    "        'line_id',\n",
    "        'text',\n",
    "        'text_reply',\n",
    "        'line_text_len',\n",
    "        'line_reply_len'\n",
    "    ]\n",
    "\n",
    "    return df_[columns]\n",
    "    \n",
    "    \n",
    "gg_clean = clean_dataset_gg(gg, thr = 20)\n",
    "gg_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "movies = pd.read_csv(\"cornell_movie_dialogs_corpus/movie_titles_metadata.txt\",sep=\" \\+\\+\\+\\$\\+\\+\\+ \",\n",
    "                     engine=\"python\", header=None, names = [\"movie_id\",\"title\",\"year\",\"imdb_rating\",\"imdb_votes\",\"genres\"])\n",
    "text = pd.read_csv(\"cornell_movie_dialogs_corpus/movie_lines.txt\", sep=\" \\+\\+\\+\\$\\+\\+\\+ \", \n",
    "                   engine=\"python\", header=None, names = [\"line_id\",\"char_id\",\"movie_id\",\"char_name\",\"text\"], \n",
    "                  quoting=csv.QUOTE_NONE)\n",
    "struct = pd.read_csv(\"cornell_movie_dialogs_corpus/movie_conversations.txt\", sep=\" \\+\\+\\+\\$\\+\\+\\+ \", \n",
    "                     engine=\"python\", header=None, names = [\"char_id_1\",\"char_id_2\",\"movie_id\",\"utterances\"])\n",
    "\n",
    "\n",
    "struct_proc = struct.reset_index().rename(index=str, columns={\"index\":\"dialogue\"})\n",
    "struct_proc.utterances = struct_proc.utterances.apply(lambda x: eval(x))\n",
    "s = struct_proc.utterances.apply(lambda x: pd.Series(x)).stack().reset_index(level=[0,1])\n",
    "s = s.rename(index=str,columns={\"level_0\":\"dialogue\", \"level_1\":\"dialogue_order\", 0:\"line_id\"})\n",
    "s.dialogue = s.dialogue.astype(int)\n",
    "struct_proc = pd.merge(struct_proc.drop(\"utterances\",axis=1),s,how=\"right\",on=\"dialogue\")\n",
    "\n",
    "\n",
    "# MERGE DATA\n",
    "cornell_movie = pd.merge(struct_proc,text,on=[\"line_id\", \"movie_id\"],how=\"inner\")\n",
    "print(cornell_movie.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset_cornell(df, thr):\n",
    "    \n",
    "    df_ = df.copy()\n",
    "    \n",
    "    # clean and uniformize text\n",
    "    df_['text'] = df_['text'].fillna('')\n",
    "    df_['text'] = df_.text.apply(clean_text)\n",
    "    \n",
    "    \n",
    "    # build responses for each dialogue line (response is next dialogue)\n",
    "    df_ = df_.sort_values(by = ['dialogue'])\n",
    "    df_ = build_reply(df = df_, scene_key = 'dialogue', line_col = 'text')\n",
    "    \n",
    "    \n",
    "    # calculate number of words in each line and response \n",
    "    df_['line_text_len'] = count_words(df_.text)\n",
    "    df_['line_reply_len'] = count_words(df_.text_reply)\n",
    "    \n",
    "    \n",
    "    # filter out the dialogues with lengths that exceed the threshold\n",
    "    df_ = filter_long_dialogues(df_, 'line_text_len', 'line_reply_len', thr)\n",
    "    \n",
    "    df_['show'] = 'cornell_movies'\n",
    "    \n",
    "    # select relevant columns\n",
    "    columns = [\n",
    "        'show',\n",
    "        'dialogue',\n",
    "        'movie_id',\n",
    "        'dialogue_order',\n",
    "        'line_id',\n",
    "        'text',\n",
    "        'text_reply',\n",
    "        'line_text_len',\n",
    "        'line_reply_len'\n",
    "    ]\n",
    "\n",
    "    return df_[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornell_movie_clean = clean_dataset_cornell(cornell_movie, 20)\n",
    "cornell_movie_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornell_movie.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['show', 'line_id', 'text', 'text_reply', 'line_text_len', 'line_reply_len']\n",
    "\n",
    "dataset = pd.concat([\n",
    "    friends_clean[cols],\n",
    "    the_office_clean[cols],\n",
    "    gg_clean[cols],\n",
    "    cornell_movie_clean[cols]], axis = 0, ignore_index = True)\n",
    "\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('comedy_data/dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_clean(text):\n",
    "\n",
    "    text = text.replace(\"...\",\"\")\n",
    "    text = text.replace(\".\",\" .\")\n",
    "    text = text.replace(\"!\",\" !\")\n",
    "    text = text.replace(\"?\",\" ?\")\n",
    "\n",
    "    return text\n",
    "\n",
    "dialogues = dataset.copy()\n",
    "dialogues['text_1'] = dialogues['text'].apply(lambda t: extra_clean(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = pd.read_csv('comedy_data/dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
