{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "st = StanfordNERTagger('stanford-ner-4.0.0/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                       'stanford-ner-4.0.0/stanford-ner.jar',\n",
    "                       encoding='utf-8')\n",
    "\n",
    "\n",
    "def replace_tags(text):\n",
    "    \n",
    "    tokenized_text = word_tokenize(text)\n",
    "    classified_text = st.tag(tokenized_text)\n",
    "    \n",
    "    person = [pair[0] for pair in classified_text if pair[1] == 'PERSON']\n",
    "    location = [pair[0] for pair in classified_text if pair[1] == 'LOCATION']\n",
    "    organization = [pair[0] for pair in classified_text if pair[1] == 'ORGANIZATION']\n",
    "        \n",
    "        \n",
    "    text = text.replace('<', '').replace('>', '')\n",
    "    for p in person:\n",
    "        text = text.replace(p, '<person>')\n",
    "    for l in location:\n",
    "        text = text.replace(l, '<location>')\n",
    "    for o in organization:\n",
    "        text = text.replace(o, '<organization>')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text) # exclude content between () and []\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\" re \", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'em\", \" them\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"'cuz\", \"because\", text)\n",
    "    text = re.sub(r\"'cos\", \"because\", text)\n",
    "    text = re.sub(r\"wanna\", \"want\", text)\n",
    "    text = re.sub(r\"d'you\", \"do you\", text)\n",
    "    text = re.sub(r\"d'ya\", \"do you\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"gimme\", \"give me\", text)\n",
    "    text = re.sub(r\"lemme\", \"let me\", text)\n",
    "    text = re.sub(r\"gonna\", \"going to\", text)    \n",
    "    text = re.sub(r\"ya\", \"you\", text)\n",
    "    text = re.sub(r\"yknow\", \"you know\", text)\n",
    "    text = re.sub(r\"\\x96\", \"\", text)\n",
    "    text = re.sub(r\"\\x91\", \"\", text)\n",
    "    text = re.sub(r\"you�re\", \"you are\", text)\n",
    "    text = re.sub(r\"don�t\", \"do not\", text)\n",
    "    text = re.sub(r\"it�s\", \"it is\", text)\n",
    "    text = re.sub(r\"i�m\", \"i am\", text)\n",
    "    text = re.sub(r\"that�s\", \"that is\", text)\n",
    "    text = re.sub(r\"what�s\", \"what is\", text)\n",
    "    text = re.sub(r\"didn�t\", \"did not\", text)\n",
    "    text = re.sub(r\"he�s\", \"he is\", text)\n",
    "    text = re.sub(r\"�\", \"\", text)\n",
    "    text = re.sub(r\"maam\", \"madam\", text)\n",
    "    text = re.sub(r\"kinda\", \"kind of\", text)\n",
    "    text = re.sub(r\"sorta\", \"sort of\", text)\n",
    "    text = re.sub(r\"outta\", \"out of\", text)\n",
    "    text = re.sub(r\"dunno\", \"do not know\", text)\n",
    "    text = re.sub(r\"betcha\", \"bet you\", text)\n",
    "    text = re.sub(r\"gotcha\", \"got you\", text)\n",
    "    text = re.sub(r\"coulda\", \"could have\", text)\n",
    "    text = re.sub(r\"woulda\", \"would have\", text)\n",
    "    text = re.sub(r\"cuppa\", \"cup of\", text)\n",
    "    text = re.sub(r\"whassup\", \"what is up\", text)\n",
    "    text = re.sub(r\"-\", \" \", text) \n",
    "    text = re.sub(r\"[-¯()\\\"#/@;:<>{}`+=~|,']\", \"\", text) # remove special characters. keep ponctuation (.!?)\n",
    "    text = re.sub(r\"cmon\", \"come on\", text)\n",
    "    text = re.sub(r\"  \", \" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seinfeld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://data.world/juanjosecas/seinfeld-scripts\n",
    "seinfeld = pd.read_csv('comedy_data/seinfeld_scripts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, download\n",
    "corpus = Corpus(filename=download(\"friends-corpus\"))\n",
    "\n",
    "corpus_df = corpus.get_utterances_dataframe()\n",
    "corpus_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(series):\n",
    "    return series.str.split(' ').apply(len)\n",
    "\n",
    "def build_reply(df, scene_key, line_col):\n",
    "    # a dialogue reply is the next dialogue in the same scene\n",
    "    df_ = df.copy()\n",
    "    df_['text_reply'] = df_.groupby(scene_key)[line_col].shift(-1)\n",
    "    df_ = df_.dropna(subset = ['text_reply'])\n",
    "    return df_\n",
    "    \n",
    "def filter_long_dialogues(df, line_col_len, reply_col_len, thr):\n",
    "    df_ = df.copy()\n",
    "    df_['length_is_ok'] = ((df_[line_col_len] <= thr) & \n",
    "                           (df_[line_col_len] > 1) &\n",
    "                           (df_[reply_col_len] <= thr) &\n",
    "                           (df_[reply_col_len] > 1))\n",
    "    df_ = df_[df_.length_is_ok]\n",
    "    return df_.drop(['length_is_ok'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset_friends(df, thr):\n",
    "    \n",
    "    df_ = df.copy()\n",
    "    \n",
    "    # clean and uniformize text\n",
    "    df_.text = df_.text.apply(replace_tags)\n",
    "    df_.text = df_.text.apply(clean_text)\n",
    "    \n",
    "    # build reply\n",
    "    replies = df_[['reply_to','text']].rename(columns = {'text': 'text_reply', 'reply_to': 'id'})\n",
    "    replies = replies.reset_index(drop = True).dropna()\n",
    "\n",
    "    df_ = df_.reset_index().dropna(subset = ['text'])\n",
    "    df_ = df_.merge(replies, on = 'id', how = 'inner')\n",
    "    \n",
    "    \n",
    "    # calculate number of words in each line and response \n",
    "    df_['line_text_len'] = count_words(df_.text)\n",
    "    df_['line_reply_len'] = count_words(df_.text_reply)\n",
    "    \n",
    "    # filter out the dialogues with lengths that exceed the threshold\n",
    "    df_ = filter_long_dialogues(df_, 'line_text_len', 'line_reply_len', thr)\n",
    "    \n",
    "    df_['show'] = 'friends'\n",
    "    \n",
    "    df_ = df_.rename(columns = {'id':'line_id'})\n",
    "    \n",
    "    # select relevant columns\n",
    "    columns = [\n",
    "        'show',\n",
    "        'line_id',\n",
    "        'text',\n",
    "        'text_reply',\n",
    "        'line_text_len',\n",
    "        'line_reply_len'\n",
    "    ]\n",
    "\n",
    "    return df_[columns]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_clean = clean_dataset_friends(corpus_df, thr = 20)\n",
    "friends_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://data.world/abhinavr8/the-office-scripts-dataset\n",
    "the_office = pd.read_csv('comedy_data/the_office_scripts.csv')\n",
    "    \n",
    "\n",
    "def clean_dataset_office(df, thr):\n",
    "    \n",
    "    df_ = df.copy()\n",
    "    \n",
    "    # clean and uniformize text\n",
    "    df_.line_text = df_.line_text.apply(clean_text)\n",
    "\n",
    "    # create key for scenes\n",
    "    df_['scene_key'] = ('s' + df_.season.astype(str).str.zfill(2) + '_' +\n",
    "                        'e' + df_.episode.astype(str).str.zfill(2) + '_' +\n",
    "                        'c' + df_.scene.astype(str).str.zfill(2))\n",
    "    \n",
    "    \n",
    "    # create index for dialogue order\n",
    "    df_['dialogue_order'] = df_.groupby('scene_key')['line_text'].cumcount()\n",
    "    \n",
    "    # create key for dialogue utterance\n",
    "    df_['line_id'] = df_['scene_key'] + '_u' + df_.dialogue_order.astype(str).str.zfill(2)\n",
    "\n",
    "\n",
    "    # build responses for each dialogue line (response is next dialogue)\n",
    "    df_ = build_reply(df_, 'scene_key', 'line_text')\n",
    "    \n",
    "    \n",
    "    # remove dialogues with typos\n",
    "    df_['has_typo'] = (df_.line_text.str.contains('���')) | (df_.text_reply.str.contains('���'))\n",
    "    df_ = df_[~df_.has_typo].reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "    # calculate number of words in each line and response \n",
    "    df_['line_text_len'] = count_words(df_.line_text)\n",
    "    df_['line_reply_len'] = count_words(df_.text_reply)\n",
    "    \n",
    "    # filter out the dialogues with lengths that exceed the threshold\n",
    "    df_ = filter_long_dialogues(df_, 'line_text_len', 'line_reply_len', thr)\n",
    "    \n",
    "    df_['show'] = 'the_office'\n",
    "    \n",
    "    # select relevant columns\n",
    "    columns = [\n",
    "        'show',\n",
    "        'season',\n",
    "        'episode',\n",
    "        'scene',\n",
    "        'scene_key',\n",
    "        'dialogue_order',\n",
    "        'line_id',\n",
    "        'line_text',\n",
    "        'text_reply',\n",
    "        'line_text_len',\n",
    "        'line_reply_len'\n",
    "    ]\n",
    "\n",
    "    return df_[columns].rename(columns = {'line_text':'text'})\n",
    "    \n",
    "    \n",
    "the_office_clean = clean_dataset_office(the_office, thr = 20)\n",
    "the_office_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_office_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### himym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "himym = pd.read_csv('comedy_data/himym.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "himym.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gilmore Girls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = pd.read_csv('comedy_data/gilmore_girls.csv')\n",
    "\n",
    "\n",
    "def clean_dataset_gg(df, thr):\n",
    "    \n",
    "    df_ = df.copy()\n",
    "    \n",
    "    df_ = df_.drop('scene', axis = 1).rename(columns = {'dialogues':'scene', 'sequence':'dialogue_order'})\n",
    "    df_['season'] = df_.episode.apply(lambda text: int(text.split('x')[0]))\n",
    "    df_['episode'] = df_.episode.apply(lambda text: int(text.split('x')[1].split('-')[0]))\n",
    "    \n",
    "\n",
    "    \n",
    "    # clean and uniformize text\n",
    "    df_['text'] = df_['line'].fillna('')\n",
    "    df_['text'] = df_.text.apply(clean_text)\n",
    "\n",
    "    # create key for scenes\n",
    "    df_['scene_key'] = ('s' + df_.season.astype(str).str.zfill(2) + '_' +\n",
    "                        'e' + df_.episode.astype(str).str.zfill(2) + '_' +\n",
    "                        'c' + df_.scene.astype(str).str.zfill(2))\n",
    "    \n",
    "    # create key for dialogue utterance\n",
    "    df_['line_id'] = df_['scene_key'] + '_u' + df_.dialogue_order.astype(str).str.zfill(2)\n",
    "    \n",
    "    df_ = df_.sort_values(by = ['season', 'episode', 'scene', 'dialogue_order'])\n",
    "    \n",
    "    # build responses for each dialogue line (response is next dialogue)\n",
    "    df_ = build_reply(df_, 'scene_key', 'text')\n",
    "    \n",
    "    \n",
    "    # calculate number of words in each line and response \n",
    "    df_['line_text_len'] = count_words(df_.text)\n",
    "    df_['line_reply_len'] = count_words(df_.text_reply)\n",
    "    \n",
    "    # filter out the dialogues with lengths that exceed the threshold\n",
    "    df_ = filter_long_dialogues(df_, 'line_text_len', 'line_reply_len', thr)\n",
    "    \n",
    "    df_['show'] = 'gilmore_girls'\n",
    "    \n",
    "    # select relevant columns\n",
    "    columns = [\n",
    "        'show',\n",
    "        'season',\n",
    "        'episode',\n",
    "        'scene',\n",
    "        'scene_key',\n",
    "        'dialogue_order',\n",
    "        'line_id',\n",
    "        'text',\n",
    "        'text_reply',\n",
    "        'line_text_len',\n",
    "        'line_reply_len'\n",
    "    ]\n",
    "\n",
    "    return df_[columns]\n",
    "    \n",
    "    \n",
    "gg_clean = clean_dataset_gg(gg, thr = 20)\n",
    "gg_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "movies = pd.read_csv(\"cornell_movie_dialogs_corpus/movie_titles_metadata.txt\",sep=\" \\+\\+\\+\\$\\+\\+\\+ \",\n",
    "                     engine=\"python\", header=None, names = [\"movie_id\",\"title\",\"year\",\"imdb_rating\",\"imdb_votes\",\"genres\"])\n",
    "text = pd.read_csv(\"cornell_movie_dialogs_corpus/movie_lines.txt\", sep=\" \\+\\+\\+\\$\\+\\+\\+ \", \n",
    "                   engine=\"python\", header=None, names = [\"line_id\",\"char_id\",\"movie_id\",\"char_name\",\"text\"], \n",
    "                  quoting=csv.QUOTE_NONE)\n",
    "struct = pd.read_csv(\"cornell_movie_dialogs_corpus/movie_conversations.txt\", sep=\" \\+\\+\\+\\$\\+\\+\\+ \", \n",
    "                     engine=\"python\", header=None, names = [\"char_id_1\",\"char_id_2\",\"movie_id\",\"utterances\"])\n",
    "\n",
    "\n",
    "struct_proc = struct.reset_index().rename(index=str, columns={\"index\":\"dialogue\"})\n",
    "struct_proc.utterances = struct_proc.utterances.apply(lambda x: eval(x))\n",
    "s = struct_proc.utterances.apply(lambda x: pd.Series(x)).stack().reset_index(level=[0,1])\n",
    "s = s.rename(index=str,columns={\"level_0\":\"dialogue\", \"level_1\":\"dialogue_order\", 0:\"line_id\"})\n",
    "s.dialogue = s.dialogue.astype(int)\n",
    "struct_proc = pd.merge(struct_proc.drop(\"utterances\",axis=1),s,how=\"right\",on=\"dialogue\")\n",
    "\n",
    "\n",
    "# MERGE DATA\n",
    "cornell_movie = pd.merge(struct_proc,text,on=[\"line_id\", \"movie_id\"],how=\"inner\")\n",
    "print(cornell_movie.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset_cornell(df, thr):\n",
    "    \n",
    "    df_ = df.copy()\n",
    "    \n",
    "    # clean and uniformize text\n",
    "    df_['text'] = df_['text'].fillna('')\n",
    "    df_['text'] = df_.text.apply(clean_text)\n",
    "    \n",
    "    \n",
    "    # build responses for each dialogue line (response is next dialogue)\n",
    "    df_ = df_.sort_values(by = ['dialogue'])\n",
    "    df_ = build_reply(df = df_, scene_key = 'dialogue', line_col = 'text')\n",
    "    \n",
    "    \n",
    "    # calculate number of words in each line and response \n",
    "    df_['line_text_len'] = count_words(df_.text)\n",
    "    df_['line_reply_len'] = count_words(df_.text_reply)\n",
    "    \n",
    "    \n",
    "    # filter out the dialogues with lengths that exceed the threshold\n",
    "    df_ = filter_long_dialogues(df_, 'line_text_len', 'line_reply_len', thr)\n",
    "    \n",
    "    df_['show'] = 'cornell_movies'\n",
    "    \n",
    "    # select relevant columns\n",
    "    columns = [\n",
    "        'show',\n",
    "        'dialogue',\n",
    "        'movie_id',\n",
    "        'dialogue_order',\n",
    "        'line_id',\n",
    "        'text',\n",
    "        'text_reply',\n",
    "        'line_text_len',\n",
    "        'line_reply_len'\n",
    "    ]\n",
    "\n",
    "    return df_[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornell_movie_clean = clean_dataset_cornell(cornell_movie, 20)\n",
    "cornell_movie_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornell_movie.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['show', 'line_id', 'text', 'text_reply', 'line_text_len', 'line_reply_len']\n",
    "\n",
    "dataset = pd.concat([\n",
    "    friends_clean[cols],\n",
    "    the_office_clean[cols],\n",
    "    gg_clean[cols],\n",
    "    cornell_movie_clean[cols]], axis = 0, ignore_index = True)\n",
    "\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('comedy_data/dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_clean(text):\n",
    "\n",
    "    text = text.replace(\"...\",\"\")\n",
    "    text = text.replace(\".\",\" .\")\n",
    "    text = text.replace(\"!\",\" !\")\n",
    "    text = text.replace(\"?\",\" ?\")\n",
    "\n",
    "    return text\n",
    "\n",
    "dialogues = dataset.copy()\n",
    "dialogues['text_1'] = dialogues['text'].apply(lambda t: extra_clean(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = pd.read_csv('comedy_data/dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
