{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import date\n",
    "import datetime as dt\n",
    "import time\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import progressbar\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add geckodriver path\n",
    "import sys\n",
    "path = '/Users/catarina/Projects/bin'\n",
    "sys.path.append(path)\n",
    "\n",
    "# 755 is the default numerical permission for files in usr/bin\n",
    "# webdriver needs a numerical permission equivalent to or greater than 755\n",
    "import os\n",
    "os.chmod(path,755) #664\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "# torexe = os.popen(r'C:\\Users\\AtechM_03\\Desktop\\Tor Browser\\Browser\\TorBrowser\\Tor\\tor.exe')\n",
    "torexe = os.popen('/usr/local/Cellar/tor/0.4.0.5_1/bin/tor.exe')\n",
    "\n",
    "# profile = FirefoxProfile(r'C:\\Users\\AtechM_03\\Desktop\\Tor Browser\\B/rowser\\TorBrowser\\Data\\Browser\\profile.default')\n",
    "profile = FirefoxProfile('/Users/catarina/Library/Application Support/TorBrowser-Data/Browser/4dgz6wzb.default')\n",
    "\n",
    "profile.set_preference('network.proxy.type', 1)\n",
    "profile.set_preference('network.proxy.socks', '127.0.0.1')\n",
    "profile.set_preference('network.proxy.socks_port', 9150)\n",
    "profile.set_preference(\"network.proxy.socks_remote_dns\", False)\n",
    "profile.update_preferences()\n",
    "\n",
    "# don't open browser\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tor is working\n",
    "\n",
    "driver = webdriver.Firefox(firefox_profile = profile, executable_path = path + '/geckodriver')\n",
    "driver.get(\"http://check.torproject.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_content(website):\n",
    "        \n",
    "    driver = webdriver.Firefox(options = options, \n",
    "                               firefox_profile = profile, \n",
    "                               executable_path = path + '/geckodriver')\n",
    "    \n",
    "    driver.get(website)\n",
    "\n",
    "    content = driver.page_source\n",
    "    \n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    return soup\n",
    "\n",
    "\n",
    "\n",
    "def get_all_urls_for_tv_show(soup):\n",
    "    \n",
    "    urls_index = []\n",
    "\n",
    "    for url in soup.findAll('a'):\n",
    "\n",
    "        url = url['href']\n",
    "\n",
    "        if (url.startswith('./viewforum')) and ('&start=' in url):\n",
    "\n",
    "            url = 'http://transcripts.foreverdreaming.org' + url[1:].split('&')[0] + '&' + url.split('&')[-1]\n",
    "\n",
    "            if url not in urls_index:\n",
    "                \n",
    "                urls_index.append(url)\n",
    "                \n",
    "    return urls_index\n",
    "\n",
    "\n",
    "def get_episodes_urls(soup):\n",
    "\n",
    "    episodes = []\n",
    "    urls = []\n",
    "\n",
    "    for item in soup.findAll('a', attrs={'class': 'topictitle'}):\n",
    "\n",
    "        if item.text.startswith('Board Updates') or item.text.startswith('Online Store'):\n",
    "            continue\n",
    "\n",
    "        url = 'http://transcripts.foreverdreaming.org' + item['href'][1:].split('&sid')[0]\n",
    "\n",
    "        episodes.append(item.text)\n",
    "        urls.append(url)\n",
    "        \n",
    "    return pd.DataFrame({'episode': episodes, 'url': urls})\n",
    "\n",
    "\n",
    "\n",
    "def get_script_tags(soup):\n",
    "    \n",
    "    script_tags = []\n",
    "    \n",
    "    for p in soup.findAll(['p', 'hr']):\n",
    "        if len(p.attrs) > 0:\n",
    "            continue\n",
    "        if p.find('a'):\n",
    "            continue\n",
    "        script_tags.append(p)\n",
    "        \n",
    "    return script_tags\n",
    "\n",
    "\n",
    "\n",
    "def parse_script(script_tags):\n",
    "\n",
    "    # initialize counters\n",
    "    scene = 1\n",
    "    dialogue = 1\n",
    "    sequence = 1\n",
    "\n",
    "    scenes = []\n",
    "    dialogues = []\n",
    "    sequences = []\n",
    "    characters = []\n",
    "    lines = []\n",
    "\n",
    "    for tag in script_tags:\n",
    "\n",
    "        content = tag.text\n",
    "        \n",
    "        # exclude content between parenthesis and brackets (non verbal information)\n",
    "        content = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", content.strip())\n",
    "        \n",
    "        # check if content is speech\n",
    "        is_speech = \":\" in content\n",
    "\n",
    "                    \n",
    "        # check if there is a change of dialogue (non dialogue text is interpreted as a change of context)\n",
    "        if (is_speech == False) & (sequence > 1):\n",
    "            dialogue += 1\n",
    "            sequence = 1\n",
    "            continue\n",
    "            \n",
    "        elif len(content) == 0:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # parse the text by splitting character and speech on the ':' \n",
    "        # e.g. of text (\"Chandler: Could this be any more amusing?\")\n",
    "        if is_speech:\n",
    "\n",
    "            character = content.split(':')[0].strip()\n",
    "            speech = \" \".join(content.split(':')[1:])\n",
    "            speech = speech.strip()\n",
    "\n",
    "\n",
    "            scenes.append(scene)\n",
    "            dialogues.append(dialogue)\n",
    "            sequences.append(sequence)\n",
    "            characters.append(character)\n",
    "            lines.append(speech)\n",
    "\n",
    "\n",
    "            sequence += 1\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame({'scene': scenes, \n",
    "                       'dialogues': dialogues,\n",
    "                       'sequence': sequences, \n",
    "                       'character': characters, \n",
    "                       'line': lines})\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def scrape_list_of_episodes(tv_show_url):\n",
    "    \n",
    "    soup = get_soup_content(tv_show_url)\n",
    "    \n",
    "    \n",
    "    # get the urls from all the episodes\n",
    "    \n",
    "    other_urls = get_all_urls_for_tv_show(soup) # get all page urls with episodes from tv show\n",
    "    \n",
    "    episodes = get_episodes_urls(soup) # get episodes from current page\n",
    "    \n",
    "    \n",
    "    for url in other_urls: # get episodes listed in other pages\n",
    "        \n",
    "        soup = get_soup_content(url)\n",
    "        \n",
    "        add_episodes = get_episodes_urls(soup)\n",
    "        \n",
    "        episodes = pd.concat([episodes, add_episodes], axis = 0, ignore_index = True)\n",
    "     \n",
    "    \n",
    "    return episodes\n",
    "        \n",
    "\n",
    "def scrape_dialogues(episodes):\n",
    "        \n",
    "    # get dialogues for each episode\n",
    "        \n",
    "    bar = progressbar.ProgressBar(maxval = episodes.shape[0],\n",
    "                                  widgets = [progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    \n",
    "    bar.start()\n",
    "    \n",
    "    \n",
    "    try:\n",
    "    \n",
    "        dialogues = pd.DataFrame()\n",
    "        \n",
    "        i = 0\n",
    "\n",
    "        for index, row in episodes.iterrows():\n",
    "\n",
    "            soup = get_soup_content(row.url)\n",
    "\n",
    "            script_tags = get_script_tags(soup)\n",
    "\n",
    "            dialogue_ep = parse_script(script_tags)\n",
    "\n",
    "            dialogue_ep['episode'] = row.episode\n",
    "\n",
    "            dialogues = pd.concat([dialogues, dialogue_ep], axis = 0, ignore_index = True) \n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "            bar.update(i)\n",
    "                \n",
    "            time.sleep(3)\n",
    "            \n",
    "            \n",
    "    except Exception as ex:\n",
    "\n",
    "        print(ex)\n",
    "        \n",
    "        return index, row.url, dialogues\n",
    "    \n",
    "    return dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_ = 'http://transcripts.foreverdreaming.org/viewforum.php?f=22'\n",
    "\n",
    "# scrape the urls of all episodes\n",
    "\n",
    "episodes = scrape_list_of_episodes(url_)\n",
    "episodes = episodes[episodes.episode != 'Gilmore Girls Transcript Index']\n",
    "episodes['season'] = episodes.episode.apply(lambda t: int(t.split('x')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the dialogues from the episodes\n",
    "\n",
    "dialogues = scrape_dialogues(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case there is a timeout in the requests from the function above, recover the dialogues scraped so far\n",
    "\n",
    "error_index, error_row, error_dialogues = dialogues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
