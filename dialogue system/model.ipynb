{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, LSTM, Dense, TimeDistributed, RepeatVector, Bidirectional, Embedding\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "\n",
    "import csv\n",
    "\n",
    "from nltk import tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from scipy.sparse import coo_matrix, hstack, csr_matrix\n",
    "import sparse\n",
    "\n",
    "from tensorflow.sparse import SparseTensor, to_dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('comedy_data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show</th>\n",
       "      <th>line_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_reply</th>\n",
       "      <th>line_text_len</th>\n",
       "      <th>line_reply_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>friends</td>\n",
       "      <td>s01_e01_c01_u001</td>\n",
       "      <td>there is nothing to tell! he is just some guy ...</td>\n",
       "      <td>come on you are going out with the guy! there ...</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>friends</td>\n",
       "      <td>s01_e01_c01_u002</td>\n",
       "      <td>come on you are going out with the guy! there ...</td>\n",
       "      <td>all right joey be nice. so does he have a hump...</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>friends</td>\n",
       "      <td>s01_e01_c01_u003</td>\n",
       "      <td>all right joey be nice. so does he have a hump...</td>\n",
       "      <td>wait does he eat chalk?</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>friends</td>\n",
       "      <td>s01_e01_c01_u011</td>\n",
       "      <td>oh yeah. had that dream.</td>\n",
       "      <td>then i look down and i realize there is a phon...</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>friends</td>\n",
       "      <td>s01_e01_c01_u012</td>\n",
       "      <td>then i look down and i realize there is a phon...</td>\n",
       "      <td>instead of...?</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      show           line_id  \\\n",
       "0  friends  s01_e01_c01_u001   \n",
       "1  friends  s01_e01_c01_u002   \n",
       "2  friends  s01_e01_c01_u003   \n",
       "3  friends  s01_e01_c01_u011   \n",
       "4  friends  s01_e01_c01_u012   \n",
       "\n",
       "                                                text  \\\n",
       "0  there is nothing to tell! he is just some guy ...   \n",
       "1  come on you are going out with the guy! there ...   \n",
       "2  all right joey be nice. so does he have a hump...   \n",
       "3                           oh yeah. had that dream.   \n",
       "4  then i look down and i realize there is a phon...   \n",
       "\n",
       "                                          text_reply  line_text_len  \\\n",
       "0  come on you are going out with the guy! there ...             13   \n",
       "1  all right joey be nice. so does he have a hump...             17   \n",
       "2                            wait does he eat chalk?             16   \n",
       "3  then i look down and i realize there is a phon...              5   \n",
       "4                                     instead of...?             12   \n",
       "\n",
       "   line_reply_len  \n",
       "0              17  \n",
       "1              16  \n",
       "2               5  \n",
       "3              12  \n",
       "4               2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create final dataset with input and output sentences\n",
    "\n",
    "# final_dataset = dataset.copy()\n",
    "# final_dataset = final_dataset[['dialogue', 'text_proc']].rename(columns = {'text_proc': 'input'})\n",
    "# final_dataset['output'] = final_dataset.groupby('dialogue')['input'].shift(-1)\n",
    "# final_dataset = final_dataset.drop('dialogue', axis = 1).dropna()\n",
    "# final_dataset['output'] = final_dataset['output'].apply(lambda text: \"<GO> \" + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE VOCABULARY\n",
    "vocab_size1 = 5000-4\n",
    "content1 = dataset.input.values\n",
    "regex_pattern = '([a-z]+|[0-9]+|\\\\b[?!.]+|[\\x27]\\\\b)'\n",
    "vectorizer1= CountVectorizer(token_pattern=regex_pattern,max_features=vocab_size1)\n",
    "vectorizer1.fit(content1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, regex_pattern = '([a-z]+|[0-9]+|\\\\b[?!.]+|[\\x27]\\\\b)'):\n",
    "    return tokenize.regexp_tokenize(text.lower(),pattern=regex_pattern)\n",
    "\n",
    "def rare_words_to_unk(tokenlist, vocab=vectorizer1.vocabulary_.keys(), replaceToken='<UNK>'):\n",
    "    return [w if w in vocab else replaceToken for w in tokenlist]\n",
    "\n",
    "def add_go_eos_and_padding(tokenlist, max_sentence_length):\n",
    "    return tokenlist + [\"<EOS>\"] + [\"<PAD>\"] * (max_sentence_length - len(tokenlist))\n",
    "\n",
    "def tokens_to_text(tokenlist):\n",
    "    return \" \".join(tokenlist)\n",
    "\n",
    "def process_tokens(tokenlist, max_sentence_length):\n",
    "    tokenlist = rare_words_to_unk(tokenlist)\n",
    "    tokenlist = add_go_eos_and_padding(tokenlist, max_sentence_length)\n",
    "    proc_text = tokens_to_text(tokenlist)\n",
    "    return proc_text\n",
    "\n",
    "\n",
    "dataset['input_tokens'] = dataset.input.apply(tokenize_text)\n",
    "max_sentence_length = dataset[\"input_tokens\"].apply(len).max()\n",
    "dataset['text_proc'] = dataset.input_tokens.apply(lambda tokenlist: process_tokens(tokenlist, max_sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final dataset with input and output sentences\n",
    "\n",
    "final_dataset = dataset.copy()\n",
    "final_dataset = final_dataset[['dialogue', 'text_proc']].rename(columns = {'text_proc': 'input'})\n",
    "final_dataset['output'] = final_dataset.groupby('dialogue')['input'].shift(-1)\n",
    "final_dataset = final_dataset.drop('dialogue', axis = 1).dropna()\n",
    "final_dataset['output'] = final_dataset['output'].apply(lambda text: \"<GO> \" + text)\n",
    "\n",
    "\n",
    "final_dataset = final_dataset[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the sentences\n",
    "\n",
    "def integer_encoding(docs):\n",
    "\n",
    "    # create the tokenizer\n",
    "    t = Tokenizer(filters = '', split=\" \")\n",
    "    \n",
    "    # fit the tokenizer on the sentences\n",
    "    t.fit_on_texts(docs)\n",
    "    \n",
    "    # summarize what was learned\n",
    "    print('documents count: ', t.document_count)\n",
    "    print('vocabulary size: ', len(t.word_counts))\n",
    "\n",
    "    # integer encode sentences\n",
    "    encoded_docs = t.texts_to_sequences(docs)\n",
    "    \n",
    "    return t\n",
    "\n",
    "\n",
    "# all text sentences\n",
    "docs = pd.concat([final_dataset.input, final_dataset.output], ignore_index = True)\n",
    "\n",
    "# train encoder on complete text\n",
    "int_encoder = integer_encoding(docs)\n",
    "\n",
    "# integer encode inputs and outputs\n",
    "encoded_input = int_encoder.texts_to_sequences(final_dataset.input)\n",
    "encoded_output = int_encoder.texts_to_sequences(final_dataset.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tensor(encoder, encoded_docs):\n",
    "\n",
    "    encoder = int_encoder\n",
    "    encoded_docs = decoder_target_data\n",
    "\n",
    "    # number of documents\n",
    "    nr_docs = len(encoded_docs)\n",
    "\n",
    "    # length of sentence\n",
    "    len_sentence = len(encoded_docs[0])\n",
    "\n",
    "    # vocabulary size\n",
    "    vocab_size = len(encoder.word_counts)\n",
    "\n",
    "    # force integer encoding to start at 0 instead of 1\n",
    "    #     encoded_docs_ = np.array(encoded_docs) - 1\n",
    "    encoded_docs_ = encoded_docs\n",
    "\n",
    "\n",
    "    # find dimensions to build 3D sparse tensor\n",
    "    sentence = []\n",
    "    position = []\n",
    "    words_index = []\n",
    "\n",
    "    for doc_index in np.arange(0, nr_docs):\n",
    "\n",
    "        s = list(np.ones(len_sentence, dtype = int) * doc_index)\n",
    "        p = list(np.arange(0, len_sentence))\n",
    "        w = list(encoded_docs_[doc_index])\n",
    "\n",
    "        sentence += s\n",
    "        position += p\n",
    "        words_index += w\n",
    "\n",
    "\n",
    "    # build indices from dimensions\n",
    "    indices = []\n",
    "\n",
    "    for s, p, w in zip(sentence, position, words_index):\n",
    "\n",
    "        indices.append([s, p, w])\n",
    "\n",
    "\n",
    "    # build sparse tensor\n",
    "\n",
    "    data = np.ones(len_sentence * nr_docs)\n",
    "\n",
    "    t = SparseTensor(indices = indices, values = data, dense_shape = [nr_docs, len_sentence, vocab_size])\n",
    "\n",
    "    # transform to dense\n",
    "\n",
    "    t = to_dense(t)\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import convert_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index encode the sentences\n",
    "\n",
    "encoder_input_data = np.array(int_encoder.texts_to_sequences(final_dataset.input)) - 1\n",
    "print(encoder_input_data.shape)\n",
    "\n",
    "decoder_target_data = np.array(int_encoder.texts_to_sequences(final_dataset.output)) - 1\n",
    "\n",
    "print(decoder_target_data.shape)\n",
    "\n",
    "# shift the decoder input / target data so that the target predicts the next word after the input\n",
    "\n",
    "decoder_input_data = decoder_target_data[:,:-1] # remove last word\n",
    "\n",
    "decoder_target_data = decoder_target_data[:,1:] # remove first word\n",
    "\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_data_ohe = build_tensor(int_encoder, decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(int_encoder.word_index)\n",
    "print('vocabulary size: ', vocab_size)\n",
    "\n",
    "embedding_dim = 50\n",
    "\n",
    "max_length_in = encoder_input_data.shape[1]\n",
    "print(max_length_in)\n",
    "max_length_out = decoder_target_data.shape[1]\n",
    "print(max_length_out)\n",
    "\n",
    "# latent_dim = 100\n",
    "\n",
    "# batch_size = 32\n",
    "# epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embeddings\n",
    "embedding_dim = 50\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove6B/glove_6B_50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in int_encoder.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i-1] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer\n",
    "\n",
    "arguments:\n",
    "\n",
    "- input_dim: integer. size of vocabulary (i.e. maximum integer index + 1)\n",
    "- output_dim: dimension of the dense embedding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = vocab_size\n",
    "num_decoder_tokens = vocab_size\n",
    "latent_dim = embedding_dim\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "\n",
    "embedder_in = Embedding(num_encoder_tokens, # vocabulary size\n",
    "                        latent_dim, # embeddings vector size\n",
    "                        weights=[embedding_matrix],\n",
    "                        trainable = False)\n",
    "\n",
    "encoder_inputs_embedded = embedder_in(encoder_inputs)\n",
    "\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoded_lstm, state_h, state_c = encoder_lstm(encoder_inputs_embedded)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(max_length_out,))\n",
    "\n",
    "# embedder_out = Embedding(num_decoder_tokens, latent_dim)\n",
    "embedder_out = Embedding(num_decoder_tokens, # vocabulary size\n",
    "                        latent_dim, # embeddings vector size\n",
    "                        weights=[embedding_matrix],\n",
    "                        trainable = False)\n",
    "decoder_inputs_embedded = embedder_out(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_embedded, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile & run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
    "# rather than sequences of integers like `decoder_input_data`!\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data_ohe,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "\n",
    "\n",
    "decoder_inputs_ = Input(shape=(None,), batch_size = 1)\n",
    "\n",
    "x = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs_)\n",
    "\n",
    "decoder_outputs_, state_h, state_c = decoder_lstm(x, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_outputs_ = decoder_dense(decoder_outputs_)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_] + decoder_states_inputs, \n",
    "    [decoder_outputs_] + decoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_encoder.word_index['<go>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_decoder_seq_length = max_length_out\n",
    "\n",
    "# def decode_sequence(input_seq):\n",
    "\n",
    "input_seq = sentence_encoded\n",
    "\n",
    "# Encode the input as state vectors.\n",
    "states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "# Generate empty target sequence of length 1.\n",
    "# target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "target_seq = np.ones((1, max_length_out)) * (int_encoder.word_index['<pad>']-1)\n",
    "# Populate the first character of target sequence with the start character.\n",
    "#     target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "# target_seq[0, 0, int_encoder.word_index['<go>'] - 1] = 1\n",
    "target_seq[0][0] = int_encoder.word_index['<go>'] - 1\n",
    "print(target_seq)\n",
    "print()\n",
    "\n",
    "# Sampling loop for a batch of sequences\n",
    "# (to simplify, here we assume a batch of size 1).\n",
    "stop_condition = False\n",
    "decoded_sentence = ''\n",
    "n_words = 1\n",
    "while not stop_condition:\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "    \n",
    "    print(output_tokens[0, n_words, :])\n",
    "#     print(output_tokens[0, -1, :])\n",
    "\n",
    "    # Sample a token\n",
    "    sampled_token_index = np.argmax(output_tokens[0, n_words, :])\n",
    "\n",
    "    sampled_token_index += 1\n",
    "#         sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "    sampled_char = int_encoder.index_word[sampled_token_index]\n",
    "\n",
    "    decoded_sentence += \" \" + sampled_char\n",
    "    print(decoded_sentence)\n",
    "\n",
    "    # Exit condition: either hit max length\n",
    "    # or find stop character.\n",
    "    if (sampled_char == '<eos>' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "        stop_condition = True\n",
    "\n",
    "    # Update the target sequence (of length 1).\n",
    "#     target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "#     target_seq[0, 0, sampled_token_index] = 1.\n",
    "    target_seq[0][n_words] = sampled_token_index\n",
    "    print(target_seq)\n",
    "    print()\n",
    "#     target_seq = np.argmax(output_tokens, axis = 1)\n",
    "    n_words += 1\n",
    "\n",
    "    # Update states\n",
    "    states_value = [h, c]\n",
    "\n",
    "#     return target_seq, decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'hi how are you'\n",
    "\n",
    "sentence_proc = process_tokens(tokenize_text(sentence), max_sentence_length)\n",
    "sentence_proc = sentence_proc.lower()\n",
    "\n",
    "sentence_encoded = int_encoder.texts_to_sequences(pd.Series(sentence_proc))\n",
    "\n",
    "# a, b = decode_sequence(sentence_encoded)\n",
    "\n",
    "# sentence_matrix = build_tensor(int_encoder, sentence_encoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
